# Foundations of Natural Language Processing
## LMU SoSe 21
### Sentence & Document Embeddings

Lecture: Foundations of Natural Language Processing (Seminar)  
Institut: Statistik, LMU MÃ¼nchen  
Advisors: Prof. Dr. Christian Heumann, Daniel Schalk, Matthias Aßenmacher  

Topic: Sentence & Document Embeddings  
Author: Anne Gritto  
Supervisor: Matthias Aßenmacher  

The seminar gives an overview of the basic concepts of neural networks and embedding methods in NLP.  

### Abstract

Natural language processing has become a widely researched field in recent years. It describes methods to represent human language as fixed-length vectors that can be an input of machine learning algorithms. This seminar paper gives an overview of certain methods that represent sentences and documents as vectors, starting with the simple bag-of-words approach. Following that, neural network-based methods will be explained. Word embeddings that have gained a lot of popularity will shortly be addressed. Paragraph Vector then adapts that basis of word embeddings by representing not only words, but also sentences, paragraphs or documents as fixed-length vectors. Furthermore, a method for unsupervised learning of a distributed sentence encoder, Skip-Thought Vectors, will be described. The bag-of-words model and Paragraph Vector will then be applied in Python and its results will be compared. In these analysis, bag-of-words outperform Paragraph Vectors. However, it will also be pointed out how to improve this application of Paragraph Vector to gain better results. In the end, an outlook to the state-of-the-art method in sentence and documents embedding, Sentence-BERT, will be given.


